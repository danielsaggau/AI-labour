---
output:
  pdf_document:
    number_sections: true
citation_package: biblatex
title: "Importing threats and fears? - Using Latent Dirichlet Allocation to disentangle threats and fears on twitter data"
author: Daniel Saggau^[Ludwig Maximilian Unversity Munich, daniel.saggau@campus.lmu.de]
date: May 2020
fontsize: 12pt
bibliography: [bibnew.bib]
nocite: '@*'
abstract: Michael Webb developed a method to examine the impact of technology on occupation matching patent data with job descriptions. Using natural language processing tools, Webb argues that technology challenges the existing standing of different sectors, depending on the task at hand. Moreover, the effects of software, automation and artifical intelligence deviate. Webb argues that AI will reduce 90:10 wage inequality, but will not effect the top 1%. To complement this analysis, this paper tries to disentangile how the general public perceives threats to employment. Studying 170.000 tweets during the period of the economic lockdown, evidence hints towards the narrative that tweets related to trade, China and migration are loaded with a more diverse set of sentiments relative to tweets related to technology. Moreover, geospatial difference exist between topics. Topic models and a network analysis allow for a glimpse into the general standing of the public opinion. Nevertheless, descriptive data hints towards a bifurcated narrative during the pandemic. Surprisingly, technology and automation fail to substantiate as a sizable threat, as little of the communication on twitter steers towards this dicussion despite the recent call for further automation. 

---
```{r, include=FALSE}
options(tinytex.verbose = TRUE)
library(knitr)
library(tidyverse)
library(haven)
setwd("~/automation and labour economics")
img_migration_map <- "images/maps/map migration tweets.png"
comp_china <-"images/comp plot china.png"
```

# Introduction

Opinions about the the future of automation have become increasingly bifurcated, as much as political.
The dichotomous nature of technological change have made this topic convoluted.
On the hand, technology is increasing productivity and job demand.
On the other hand technology has shaped the nature of work, causing a reallocation of labour supply..
Politics has seen a visible movement towards partisanship.
The attention of policy makers has been consumed by a number of topics, failing to agree on a common narrative (Autor et al., 2020).
Technological change as been at the center of attention in research on economic growth, as manifested in several pivotal models in economic growth such as the Solow or Ramsey model.
While technology is evolving and spreading in an ubiquitous manner, the effect of technology is still largely ambitious as different breakthroughs have led to severely different outcomes.
In the last decades, software and robotics have reshaped various industries.
Low skill jobs have become redundant with the establishment of industrial robots (@webb_impact_2019).
Irrespective, other technological breakthroughs have challenged the existing standing of jobs, demanding a higher tier of skills (Autor, 2018XXX).
One example is Software,leading to a reduction in middle class jobs.
Numerous scholars have examined this effect.
Autor, Levy and Murname (2003) provided a framework to understand these outcomes.
They compare routine and non-routine tasks, suggesting that tasks which are non-routine are less exposed to software.
Interpersonal and analytical tasks have been challenging to replace by software according to their findings.
Routine manual work on the other hand has been effected the strongest, with routine cognitive tasks following second.
Due to the gender gap in the selection of jobs, women have been less effected by technological change relative to men, due their accumulation in professions in fields with stronger interpersonal components.
 Acemoglu and Restrepo (2018) build a task model to quantify exposure to technology.
@webb_impact_2019 builds on these framworks, providing a holistic examination of different types of technology.
Further, @webb_impact_2019 complements the existing literature by looking at impact of artifical intellgence and the recent leaps within these fields.

Within the field of AI there are two pivotal pillars.
General artificial intelligence deals with AI that targets universal decision making.
The second is specific artificial intelligence.
This is a type of AI that handles a specific problem, but would not be able to act outside of this framework.
One field within specific AI, machine learning, has become the flagship for artificial intelligence.
These field have gained prominence due to fear of automation caused by these technologies.
Supervised learning has become highly efficiently, accomplishing better performances than humans in numerous task.
Moreover, supervised  learning, the counterpart to unsupervised learning, is a method in which an task is defined.
Unsupervised learning deals with classification/prediction of of unknown tasks.
Here, a task is detected through training.
The application of machine learning is accelerating in fields that are targeting the employment of high skilled labour.
Prominent examples are the use of machine learning for anomaly detection in medicine and the detection of financial fraud.
Sendhil Mullainathan and Jann Spiess (201XXX7) illustrate in what manner economics has adopted machine learning methods in their paper *"Machine Learning: An Applied Econometric Approach"*.
Applications include the use of machine learning to analyse satelite data, allowing one to get insights into poverty data.
Especially in the field of economic development this method has gained attention.
Further, another application is the prediction of teacher quality in hiring decisions and the analysis of policy success.

Webb uses natural language processing to provide a holistic understanding of three foundations technological leaps, namely software, robots and artifical intelligence. 
Further, @webb_impact_2019 provides corroborating evidence that while software and robotics have a negative impact of low skilled jobs, AI is having a negative impact on high skilled labour.

Economic hardship has been part and parcel of this crisis, but attention has been drawn to the fact that this situation may stimulate further investment by large companies.
Further, as argued by Autor(XXX), during times of economic hardship partisanship and political polarization have been prevalent over the course of history.
A study by Autor(XXX) suggests that there is descriptive evidence that areas in which there were disruptive economic shocks, voting behavior has changed.
But, within public opinion there is no coherent narrative to who or what is responsible for this outcome, nor is there a consistent trajetory for future policy.
Looking at recent events, according to the department of labor, approximately 40 million people filed for unemployment in the US  over the course of a few weeks, as reported by the New York Times (2020).
The trajectory towards further automation has been amplified due to the recent pandemic.
But, who will there be to blame and how will this alter the discurse of politics and inevitably policy making?
This study will provide further insights on the specific sentiments in the COVID-19 lockdown.

This paper will use web scrapping to obtain twitter data on convoluted topics.
The study will examine data from 19.05.2020 up to the 27.05.2020, using an application programming interface (API) to obtain twitter data.
Three methods are used to assess public debate, namely sentiment analysis, a topic model and a network analysis.
To complement these insights, the appendix contains a brief geospatial outlook of tweets on automation and migration.
Sentiment analysis is an analysis of feelings, with which will allow one to manifest the general tendency towards certain topics.
Moreover, topic modelling and specifically LDA, the method used in this paper, will provide insights into the exact topics suggested in these tweets.
These insights may be able to give more precise information on the impact of the lockdown and the rise in unemployment on highly political topics.
This may allow us to get a glimpse into what direction politics and policy making may go.

This paper is structured as follows:
Firstly, there is brief introduction about inequality, technology and the role of politics and policy making for the future trajectory of an economy.
Subsequently, this paper will summarize the findings and method of the paper ' The Impact of Artificial Intelligence on the labor Market' published by Michael Webb.
Further, using twitter data, this paper will illustrate the current standing towards topics relevant for political debate using sentiment analysis.
Thereafter, these findings will be compared with the outcome of our topic model and a brief network model with trigrams will be introduced.
Lastly, this paper provides a conclusion, following a discussion, connecting recent economic and political research.

## Literature review 

The controversial release of Capital in the 21st century by Thomas Piketty (2013), provided a narrative for the dynamics of inequality.
While numerous scholars have rejected the central hypothesis in this book, it has had a large outreach.
This book was heavily inspired by the general laws of capitalism and historical materialism introduced by Karl Marx, providing a theory of history suggesting that our material conditions will shape our history.
Piketty argues that capital accumulation and technology will disrupt the forces of production and economic/political life resulting in a more inequal society (Acemoglu and Robinson, 2014).
Acemoglu and Robinson (2014) argue that this theory fails to incorporate the endogenous evolution of technology, accommodating for the fact that technology and its impact are severely dependent on the accompanying institutions and politics.
Further, they claim that one needs to disentangle the multifaceted nature of technology. 
Recent research by Acemoglu and Restrepo (2018) provide such an analysis. 
They point towards numerous countervailing forces of technological change. 
Specifically, they emphasize the role of the displacement effect and the productivity effect.
The displacement effect argues that workers are replaced through automation, because technology occupies their prior positions. 
Inevitable, this suggest a disentanglement of the relationship between wages and output, with a declining share of labor. 
The productivity effect claims that due to the reduction in production cost, economic expansion will result in accelerated demand for labor for non-automated tasks.
Acemoglu and Restrepo argue that the productivity effect could emerge in non-automated sectors or the sector which is undergoing automation.
Moreover, Acemoglu and Restrepo state that automation occurs at both the extensive margin, replacing tasks performed by workers, and the intensive margin, replacing tasks performed by machines, deepening automation.
They argue that this would also lead to further productivity but countervail the displacement effect.
Following this argument, the reinstatement effect, the emergence of novel tasks, take the same direction.
Subsequently, Acemoglu and Restrepo discuss the mismatch between technology and skills.
Their theory claims that this mismatch is created by a lack of accommodation of new technologies in the educational system, creating a lag in productivity.
To complement their model, two hypotheses of former research are introduced in their paper (Acemoglu and Restrepo ,2016). 
Firstly, there is crowding out of growth opportunities because of a unidirectional focus on specific technologies. Secondly, the authors argue that excessive automation, caused by misguided US tax incentives has led to the socially disruptive innovation, deteriorating productivity growth. 
Following this debate, @webb_impact_2019 suggests that the impact of inequality deviates between different technologies.
@webb_impact_2019 uses the static canonical task based model by @acemoglu_modeling_2018.
Historical analysis of software and robots provide support for the argument that robots have challenged numerous low skill jobs. 
This view is supported by e.g. Ayres and Miller (1983) and Acemoglu and Restrepo (2017), claiming that jobs such as manufacturing (machining, welding, painting, assembly) have been exposed to industrial robots since the sizable breakthrough in the 1980s. 
Furthermore, Webb suggests that software has challenges jobs of middle-income workers.
Webb points here to prior research Autor, Levy and Murnane (2003), which introduce routine cognitive and routine manual task scores to measure exposure.


@webb_impact_2019 argues that he makes three contriburions to the field:
1. A general-purpose measure of technology providing a more hollisitic insight relative to the prior work on specific industries and technologies 
2. Impact automation on jobs and wages
3. The impact of AI specifically

Prior studies (Edin et al. 2019) pointed out that lifetime earnings diminished for workers who were exposed to occupational decline. 

Work which relies on interpersonal components, have been less prone to automation and displacement.
Due to the gender imbalances in these jobs entailing larger components of interpersonal skills, men are more effected by technology caused displacement.
Tasks which entail a larger share of routine, have been more prone to automation and displacement.
Especially "muscle tasks", tasks which are connected to muscle work, have been exposed to robots.

# Summary

@webb_impact_2019 examined three distinct effects, namely the effect of software, robots and AI on employment.
The study uses numerous data sets:

* Patent data: Google Patents Public Data (IFI CLAIMS Patent Services)
* Job description data: O*NET database of occupations and tasks (provided by the US Department of Labour)
* Employment/Wage data: Individual level microdata form the US Census 1960-2000 and from the ACS 2000-2018 provided by IPUMS. 

Further, exposure is measure to quantify whether a task can be automated by a particular technology.
His model was based on a static task based model, originating from @acemoglu_modeling_2018.
Emphasis was placed at defining exposure of different occupations.
To determine how over the course of time technology has changed, NLP is used capture changes within job descriptions and patents.

## Model environment 

The model was introduced by @acemoglu_modeling_2018-
For more information, see this paper.

This section is a recap of the model defined by @webb_impact_2019.

The economy is defined as an entity, entailing one firm.
Good X is produced, by combining the product of different occuptations $O_{i}$.
Further here Webb introduced the assumption that elasticity of substitution is constant, manifested in the parameter $\rho$.
While this assumption does simplify the further analysis, inference based on this assumption should be questioned as the autor even admits there is evidence that this may not hold in the real world.
Hence, let:

\begin{equation}
X=\left(\sum_{i} \alpha_{i} O_{i}^{\rho}\right)^{\frac{1}{\rho}}
\end{equation}

O is the occupation, while T are the number of tasks. 

\begin{equation}
O_{i}=\left(\sum_{i} \alpha_{j} T_{i, j}^{\rho_{t}}\right)^{\frac{1}{\rho_{t}}}
\end{equation}

j indexes the number of tasks.
Now we can distinguish bvetween tasks that are exposed to automation versus tasks that are not exposed to automation.

\begin{equation}
T_{i, j}=\left\{\begin{array}{ll}
H_{i, j}+A_{i, j} R_{i, j} & \text { if automation feasible } \\
H_{i, j} & \text { otherwise }
\end{array}\right.
\end{equation}

Here R embodies machines while H stands for humans.
Further the author uses text data as fundamental input for the exposure scores.
The following section provides a comprehensive overview of how these data points were obtained.

## Method: 

Steps (see Gentzkow et al., 2019:

1. Define restrictions and deminishing the dimensionality by tokenizing the data
2. Filtering for stop words and common terms/words
3. Stemming and lemmazing the terms/words 

Natural language processing is used to process text data.
@webb_impact_2019 uses the patent data to determine the extent on the given occupation of the given technology.
The method is used to extract verb-noun pairs, also called bigrams.
Bigrams usually describe word pairs.
One can also work with unigrams, but these methods usually involve a substantial loss of information.
Ultimately, these pairs are used to quantify overlap between patents and jobs.
The method used here is to determine the frequency of different pairs.
This method approximates strongly with word embeddings.
Specifically, Webb uses a word parcing algorithm, allow one to obtain information on words within their embedded sentences.
Another notorious method is the bag of words-method, in which words are analyzed independent of their structure involving a severe loss of information and reduction in dimensionality.
While these bag of words are criticized frequently, they are often common practice.
Word parcing methods promise higher accuracy.
This method is dominant in computational linguistics but didn't establish as common practice within text data in economics.
This method is recommendable when one has substantial prior information and limited possibilities to split the data into training and test sets for text classification and generative text models.
After collecting the data, the words are lemmatised words using a dictionary method, namely the WordNet dictionary.
Lemmetisation enables one to group words into a category.
This reduces the dimensionality of the data.
Subsequently, these word pairs are used for inference.
Inference based on text data is also less prominent within research.

### Patent data

Firstly, a set of patents is selected, focusing on specific technologies.
Webb undertakes selection based on patent titles.
Subsequently, verb-noun pairs are extracted from the patent titles.
The first step is using a dependency parsing algorithm.
This algorithms provides the feature of being able to extract syntactic relationships of words within their sentences.
As mentioned by @webb_impact_2019,
For each verb, the direct object is attributed.
Further, the verbs and nouns are lemmatized.
Stop words (e.g. has, use, have) are extracted.
The probability of a specific verb-noun pair occurring is calculated.

### Job description data 

Looking at the job description data, occupations are matched with specific tasks.
The dependency parsing algorithm is used.
Tasks are ranked by their frequency in the given occupation to provide a weight for these tasks within occupations.
Each pair of verb and noun is put into conceptual categories.
For that, a hierarchy of concepts is used.
Thereafter,the author undertook stemming.
Further, probabilities are measured, generating task and occupation scores.
Due to the size of the data set, the paper could not correct for false positives.

WordNet is used to group words into hierachies of concepts.
This is done to maintain conceptual cateogries which are mutually exclusive at nature.
Further, verb-noun pairs are aggregated and used to match the patent data. 
The main regression is run using a granular aggregation level, namely WordNet level 3 (and rerun with 2 and 5 for sensitivity), to keep more information. 
For each exposure score, the weighted average is used to porduce overall scores.
Weights are based on the O*Net database, which provide frequencies of the different jobs.

<!-- Potentially this paper could use a different method to group the variables into categories -->

To complement the NLP results, an empirical strategy is selected.

\begin{equation}
r f_{c}^{t}=\frac{f_{c}^{t}}{\sum_{c \in C^{t}} f_{c}^{t}}
\end{equation}

where $r f_{c}^{t}$ is the aggregate verb noun - pairs relative frequency. 
c is the specific word-pair and t is the technology.
$f_{c}^{t}$ be the raw count.
$C^{t}$ be the full set of aggregate word pairs.

Further, followning the definition by @webb_impact_2019, let:

\begin{equation}
\text {Exposure,}_{t}=\frac{\sum_{k \in X_{i}}\left[w_{k, l} \cdot \sum_{c \in S_{k}} r f_{c}^{t}\right]}{\sum_{k \in K_{i}}\left[w_{k, l} \cdot\left\|c: c \in S_{k}\right\|\right]}
\end{equation}

define the set of task in an occupation to be $K_{i}$.
$k \in K_{i}$ is the task within the set of tasks.
$S_{k}$ is the set, entailing the verb-noun pairs.
$w_{k, l}$ is the weight of each task for occupation i.
Further specifications can be found in the original paper.



## Results for the specific technology 

For each specific technology, the author provides descriptive statistics and an empirical model.
The descriptive statistics deals with the impact on occupational wage percentile, exposure by level of education, exposure by percent of female workers in occupation and exposure by age. 
Moreover, for the empirical strategy, the dependent variable, the change in wages, is 100* change in log wage.
Wages are cells mean weekly wage for full time , full year workers in 1980.
The variable offshorability is a occupation level measure, inspired by the 2013 paper by Autor and Dorn 'The growth of low skilled service jobs and the polarization of the US labour market'.
Moreover, Webb develops an exposure measure to capture the exposure to technology.

\begin{equation}
\Delta y_{o, i, t}=\alpha_{i}+\beta \operatorname{Exp}_{o}+\gamma \mathbf{Z}_{o}+\epsilon_{o, i, t}
\end{equation}


Each section (robots, software, AI) contains information on the least and most effect occcupations, measurement result and patent selection.

### Robots:

Webb used an employment thresholf of 150 to filter out the 5 most and least exposed professions.
For reference, the list provided by Webb was included in the appendix.
Here, we filter via the number of aggregate word pairs occurances.
Moreover, the 10 most and least exposed professions are provided for all three areas. 

@webb_impact_2019 focused on industrial robots: 

Table: 

```{r, echo = FALSE}
final_df_out <- read_dta("Data /Original paper data/final_df_out.dta")
most <- final_df_out %>% 
  select(c(index, agg_pairs, robot_score)) %>%
  filter(agg_pairs >20)%>%
  arrange(desc(robot_score)) %>%
  rename(count = agg_pairs,
         score = robot_score)

kable(most[1:10,])
```

Table: 

```{r, echo =FALSE}
least <- most %>% 
  select(c(index, count, score)) %>%
  arrange(score) 

kable(least[1:10,])
```

Occupations with a substantial degree of interpersonal and manual components are less effected by robots.
One see a difference between genders, as women frequently occupy such positions.
"Muscle Jobs" on the other hand, have been vulnerable to displacements by robots.
Hence, one can see that men under the age of 30 are most effected, even more so than women in the same age group having the same educational obtainment. 

One could also provide the verbs and the respective noun pairs for the robots.

<!-- Manual selection of patents may be considered a problem in this analysis. -->

### Software:

Moreover, the third section examines the impact of AI on occupations.
AI is here narrowed down to two components namely machine learning and more specifically supervised learning and reinforced learning.
Machine learning, a method used for predictive modelling, deals foremost with making predictions based on a certain input.
Supervised learning focuses on the prediction of a supervised goal, hence we already predefined what we are trying to predict.
But what do we do when 
Reinforced learning is a method in which the algorithm learns based on feedback.
Hence, learning takes place by evaluating examples.
Reinforced learning is pivotal for autonomous cars, one of the most notorious recent projects.

```{r,echo=FALSE}
most <- final_df_out %>% 
  select(c(index, agg_pairs, software_score)) %>%
  filter(agg_pairs >20)%>%
  arrange(desc(software_score)) %>%
  rename(count = agg_pairs,
         score = software_score)

kable(most[1:10,])
```


One can see that, as mentioned by Webb 


```{r, echo=FALSE}
least <- most %>% 
  select(c(index, count, score)) %>%
  arrange(score) 

kable(least[1:10,])
```

The paper concludes that the impact of technology is not linear for the examined technologies.
While robots and software target jobs requiring low skilled labour, artificial intelligence is tackling the jobs of high skilled workers.
Hence, this study provides evidence that technological change will impact the work force for the majority of the work force and not only the lower skilled sectors, prone to replacement.

Overall, the relationship between technological automation and the labour market is considered non-linear.
As one could see in the research, the results were different for different groups.
One should note that little research was done on sectors or occupations which have benefited from further automation.
<!-- One suggestion would be to look at different sectors specifically, as this is not done here-->
The study solely focused on the negative repercussions from further automation, making predictions based on numerous assumptions and simplifications.
One of the post pivotal simplifications is the fact that elasticity of substitution is assumed to be constant at various levels.
Moreover, as mentioned in the discussion section of the paper, ownership of capital may benefit from automation.
The largest concern expressed by the author was with respect to timing.
The effect of technology on the labour market lags compared to the time the patent is instantiated.
Thus, what should have been done to optimally capture the effect of technology and labour is determining a optimal time lag for the relationship.

AI is occupied with detecting patterns, decision making and optimization.
Hence, more advanced occupations are more keen to be effect relative to classical muscle tasks. 
@webb_impact_2019 points out that people with undergraduate and graduate degrees from universities are most exposed to AI.

```{r echo=FALSE,fig.cap="Most exposed professions",fig.show='hold',fig.align='center'}

most <- final_df_out %>% 
  select(c(index, agg_pairs, ai_score)) %>%
  filter(agg_pairs >20)%>%
  arrange(desc(ai_score)) %>%
  rename(count = agg_pairs,
         score = ai_score)

kable(most[1:10,])
```

Next, we can look at the least exposed professions:

```{r echo=FALSE,fig.cap="Least exposed professions",fig.show='hold',fig.align='center'}
least <- most %>% 
  select(c(index, count, score)) %>%
  arrange(score) 

kable(least[1:10,])
```

[^2]: For Water  & Wastewater 
[^3]: Adjudicators, and Hearing Officers 
[^4]: Teachers for Computer Science, Architecture , Agricultural Sciences, Biological Science, Physics etc.

Again, we filter for aggregate word pairs occuring more than 20 times.


<!-- Numerous data sources are used. -->
<!-- The International labour organization will be used as a source for unemployment data. -->
<!-- Potentially we could also use data from the uk and do a comparative study as they have a data set on job postings during covid-19 -->

# Sentiment Analysis

Inevitably, our institutional framework and political realm will determine how a society accomodates changes in employment.
As suggested by Acemoglu and (XXX), inequality and economic trajectories are dependent on the institions and policies they are embedded in.
Hence, it is important to understand fears and threats within the general public.

Threats to work are salient within political debate. 
David Autor,David Dorn, Gordon Hanson and Kaveh Majlesi (2020) undertook a study examining how trade exposure with china has changed voting behaviour.
Their study suggests that the US has further polarized, seeing stronger movement by predominantely white non-Hispanic males moving towards the political right and minority dominated districts shifting to the political left.
As stated in their research, the polarity of ideological beliefs about the source of economic challenges and their political responses corroborate theories about in-group/out-group identification.
Rather than disentangling these arguments to polarize at a common narrative, the political sphere is moving towards a competition of group-centric resource allocation(David Autor,David Dorn, Gordon Hanson and Kaveh Majlesi, 2020)
Further, evidence indicates that trade has potentially been one of the driving factors for changing voting behavior because of the spatial density of blue color labor that has been challenged by trade agreements with China.
Technology on the other hand has altered the existing standing of both blue and white color labor. 
The authors suggest that technology in combination with other forces, has also contributed to the decline of blue color labor in these concentrated demographic and geographic areas.
While Autor et al. (XXXX) admit that evidence is merely indicative, they argue that economic and political polarization and therefore the rise of partisanship may be caused by disruptive economic shocks partially caused by trade with China. 
This effect is according to their study amplified due the manifestation of these disruptive shocks in certain geospatial areas, accelerating "vehemence at the polls"David Autor,David Dorn, Gordon Hanson and Kaveh Majlesi (2020).


Trade, international relations, technology and migration are pivotal pillars of the narrative in the political realm and shape policies.
This paper seeks to illustrate the recent sentiments towards work, using twitter data.
Ultimately, the sentiments of a society will shape the path of future voting behavior and therefore the degree of partisanship within an society.

The section of this paper is structured as follows:

## Text data in Economics

This method will provide a brief introduction into the different methods for text data usage in economics.

### Document decomposition

The bag of words-method is a frequently used approach to representing data (Grentzkow et al., 2019). 
The words within a document and decoupled from structure and order, leaving one with phrases depending on the number of words specified.
N-grams are phrases with n number of words. Social science research frequently uses unigrams, N-grams with only one word, because of the computational cost of using multiple words.
Here one has to weight dichotomous conundrum of using more words, allowing for more precision, opposed to the additional time spend.
As argued by Grentzkow et al. (2019), the recommended approach is to start with a unigram, and then depending on the outcome, and then to evaluate whether a more fine-grained approach is necessary. 

### Methods for count and attribute analysis 

Grentzkow et al. (2019), divide methods for count (ci) and attribute (vi) into four groups, namely (1) dictionary-based methods, (2) text regression methods, (3) generative language models, and (4) word embedding methods.
Further methods exist, also entailing mixed methods of these suggested methods, providing promising results. 
Numerous methods emerged for different purposes and with a varying degree of complexity. 
This paper paper will use dictionary methods and generative text models.
Figure 1 provides a brief overview, inspired by the classification used by Grentzkow et al. (2019).

```{r echo=FALSE,out.width="100%",fig.align='center',fig.cap="Overview of count and attribute methods"}
knitr::include_graphics("images/overview.png")
```

#### Dictionary Method

Dictionary methods, using a lexicon, are recommended when prior knowledge is substantial, and there are reduced opportunities to appropriately separate data into training and test data.
This method is used predominately for sentiment analysis, and has gained prominance within social sciences.
They do not involve statistical inference, as one solely specifies the estimated value of vi as a function of ci (@gentzkow_text_2019). 
Dictionaries e.g. “nrc”, provide predefined dimensions (“anger”, “fear”, etc.).
Compared to other methods, this method has the tendency to provide a wide term coverage.
Irrespective, due to the finite number of words within a lexicon and a finite number of predefined sentiments, dictionary methods are not always recommendable (Alessia D Andrea, 2015).
@gentzkow_text_2019 argue that in circumstances in which prior knowledge is substantial and there are limited possibilites to split the data intro training and test set, dictionary methods are the preferred tool of choice.

Specifically, “nrc”, “afinn” and “bing” are the three lexicons used here. “nrc” allows for the highest degree of specification when it comes to sentiments. It provides insights into “fear”, “joy”, “positive”, “negative”, “trust”, “disgust”, “anger”, “anticipation”, “surprise”, and “sadness”.  “afinn” examines values from 

While results are promising, @gentzkow_text_2019 argue that other methods could potentially outperform dictionary methods, providing a higher degree of accuracy.
One of these methods is the text regression method.  

#### Text regression Method

The text regression method is similar to the generative language model, but both differ in their starting point. 
Text regression methods starts by using the conditional expectation of the attribute vi.
note that notation needs to be adopted properly.
The data is split into training and test set, and then the learner regresses the prediction of the training set onto the test set.  
Machine learning algorithms have been one of the dominant tools within this branch of text analysis (@gentzkow_text_2019).
Also algorithms such as random forests have been promising.
But, when not being able to define the task uphand, other models such as generative text model allow for unsupervised machine learning in which no specific trajectory is defined.

#### Generative text model 

This method is currently used frequently for topic modelling.
Latent Dirichlet Allocation (LDA) is a generative probabililistic method within the field of topic modelling for text mining (Blei, et al 2003).
LDA is an unsupervised machine learning method, hence a method used for prediction of an un-specified pattern (@gentzkow_text_2019).
LDA specifically is a bayesian hierarchical model. 
Compared to hard clustering, words can embedded in a multitude of our latent factor, in this case multiple topics.

```{r echo=FALSE,out.width="89%", out.height="50%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/Blei.png")
``` 

The figure, which was originally provided by Blei et al.(2003) illustrates the process, providing further visual evidence why LDA is considered a hierarchical model.
This is the smoothed version of the LDA.
Here, alpha is a k vektor.
Theta is the proportion of the topic distribution.
Beta is the term distribution replicate.
z is the latent factor, namely the topic.
w is the word.
N is the set of topics.
M is the document in wich these objects are embedded in.
In the regular model, k and êta are not included.
The reason these factors are included is that in large document corpora, some words appear infrequently.
A corpus is a collectopn of M documents.
Maximum likelihood estimates would categories these words a 0 probability.
Hence, to accommodate for that, all words are assigned a positive probability.

Important here is that there are 3 layers.
In the first layer, the corpus level, we have alpha and beta.
In the second layer, we examine theta, being the document level.
In the third layer, we examine z and w, at the topic level.

Cluster methods such as the Dirichet-multinominal cluster method entail two layers namely the once sampled for the corpus and once for the word level.

Following the elucidation by @grun_topicmodels_2011, in the first step we need to define the term distribution $\beta$.
This indicator enables insights into the likelihood of a term occuring in a topic:

$\beta \sim$ Dirichlet $(\delta)$

Subsequently, we define the proportion of a topics for our data.
Here this parameter is defined as $\theta$ and defined as suggested by @grun_topicmodels_2011 as follows:

$\theta \sim$ Dirichlet $(\alpha)$

Further, given that we assume conditional independence between the tokenized words, we need to specificy the distribution to be multinominal distribution for the. 

\begin{equation}
z_{i} \sim \operatorname{MN}(\theta)
\end{equation}

The package allows for LDAs with both Variation expectation maximization algorithms (VEM) and Gibbs sampling.
The CTM does only allow for a VEM  algorithm, here.

Variation expectation maximization algorithm (VEM):

The expectation maximization algorithm entails two steps.
Firstly, the values of the variational parameters are optimized to define a posterior probability.
Secondly, using these posterior probabilities, we look at the lower bound of the log likelihood.
We are interested in the paramaters alpha and beta.
Specifically, we want to define the maximum likelihood estimates for each document, entailing sufficient statistics and accounting for the prior defined posterior probability.
As illustrated by Blei et al (2003)," steps are completed until the lower bound on the log likelihood converges.

\begin{equation}
\begin{aligned}
\ell(\alpha, \beta) &=\log (p(w | \alpha, \beta)) \\
&=\log \int\left\{\sum_{z}\left[\prod_{i=1}^{N} p\left(w_{i} | z_{i}, \beta\right) p\left(z_{i} | \theta\right)\right]\right\} p(\theta | \alpha) d \theta
\end{aligned}
\end{equation}


As pointed out by @gentzkow_text_2019, topic models use a variational inference.
This variational inference, allows us to get information on the lower bound of the log likelihood, which is needed for the parameter estimation.
This method is used to ensure proximity to a certain  parametric family, allowing for appproximation of the posterior probability.
In bayesian statistics, inference is estimated using a posterior probability, a probability accounting for and embedding prior knowledge.
Variation inference is a tool to accomodate the complexity of challenging posterior probabilties.
As pointed out, variation inference competes with other methods such as Markov chain Monte Carlo  sampling.
As pointed out by Blei et al. (2018): 
*"Variational inference tends to be faster and easier to scale to large data—it has been applied to problems such as large-scale document analysis, computational neuroscience, and computer vision.*
*But variational inference has been studied less rigorously than Markov chain Monte Carlo  sampling, and its statistical properties are less well understood. ".*

This method was introduced by Blei, Ng and Jordan (2003).
One of these approaches is the Kullback-Leibler (KL) divergence.
The KL divergence,also sometimes referred to as relative entropy, allows one to estimate the difference between probabilities over a common variable. 
It is similar to monte Carlo sampling.
This regreesion elaborates on how to determine the variation paramters ($\gamma^{*}, \phi^{*}$).

Next, we define the 

\begin{equation}
\left(\gamma^{*}, \phi^{*}\right)=\arg \min _{(\gamma, \phi)} \mathrm{D}_{\mathrm{KL}}(q(\theta, z | \gamma, \phi) \| p(\theta, z | w, \alpha, \beta))
\end{equation}


\begin{equation}
q(\theta, z | \gamma, \phi)=q_{1}(\theta | \gamma) \prod_{i=1}^{N} q_{2}\left(z_{i} | \phi_{i}\right)
\end{equation}


\begin{equation}
\log p(w | \alpha, \beta)=L(\gamma, \phi ; \alpha, \beta)+\mathrm{D}_{\mathrm{KL}}(q(\theta, z | \gamma, \phi) \| p(\theta, z | w, \alpha, \beta))
\end{equation}

\begin{equation}
L(\gamma, \phi ; \alpha, \beta)=\mathrm{E}_{q}[\log p(\theta, z, w | \alpha, \beta)]-\mathrm{E}_{q}[\log q(\theta, z)]
\end{equation}


#### Word Embeddings 


## Data 

This paper examines the sentiments of different threats to employment. 
Specifically, this paper studies twitter data, using a data set of 170.000 twitters. 
For each topic, 30.000 tweets are exacted, using keywords except for migration and employment for which 40.000 tweets were collected.
Subsequently, the tweets are cleaner. Keywords, numbers, websites, special signs, symbols, selected words such as the actual word itself and white spaces are removed. 
Stemming is used to derive the stem of the words. 
One does not lemmatise but this method could be use to aggregate results and reduce dimensionality in future studies.
Further, for each topic sentiments, frequency, and polarization are analyzed. 
Additionally, Latent Dirichlet Allocation (LDA) and text classifiers for each topic are used.
Last but not least, we provide a basic illustration of how to create a twitter bot algorithm. Future research could examine topic specific bots. 

## Sentiments 

This section will examine the different topics and their respective sentiments expressed on social media.
Here, we are using a dictionary method.

## Sentiments towards China

China has become a salient topic within the media due to the corona outbreak.
Trade relationships have been convoluted due to political movements in the united states.
This section will provide insights into the different sentiments towards China.
Four different dictionaries are compared.
One can see that the outcome deviates substantially.

Looking at the nrc plot, the overall sentiment score is the lowest.
This is due to the fact that the overall score compares both positive and negative feelings.
As the "nrc" dictionary is one of the broadest, words that might fall into a simple positive or negative category as done in the bing method can be displayed in a nuanced manner.
The "bing" method ranks second in overall score.
Due to the lack of further categories, it is challenging to get further insights into in which context these words were used.
The Afinn method has provided us with the a detailed glimbse into the most frequently used words, love being the only positive word.
This method also tends towards a severely negative narrative.

```{r echo=FALSE,out.width="50%", out.height="40%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("Images/China nrc.png", "images/Contribution Complex China.png"))
```

Ultimately, these results suggest that the dictionary method with unigrams is unstable.
There is reason to believe that the extend towards certain sentiments depends heavily on the method used.

```{r echo=FALSE,out.width="50%", out.height="40%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/China Afinn Value.png")
``` 
Especially the afinn method map a very much stronger negative sentiment stance towards china.
Hence, these results should be interpreted with caution.
Irrespective, all these methods also tend towards a negative narrative. 

## Sentiments towards employment 

This section analyses the sentiments during the lockdown with respect to employment.
The keyword 'employment' was used.

Again the four dictionary methods are compared.
The nrc method suggests that positive, negative, trust and anticipation are most frequently used sentiments.
Compared to the China results, the overall score is positive, with positive sentiments outperforming negative sentiments.
When examining the bing method, one can seee that positive words outperform the number of negative words in frequency.
The overall picture is coherent with the nrc method.
The afinn scores are again more balanced compared to the outcome with China.
There seems to be a more nuanced sentiment stance towards this issue, when focusing on this method.


```{r echo=FALSE,out.width="47%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/employment nrc.png","images/employment sentiment afinn.png" ))
``` 


Ultimately, the sentiment analysis for employment suggests a more nuanced stance.

```{r echo=FALSE,out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/employment bing contribution.png")
``` 

To get more specific insights, one needs to examine more concise methods such as topic modelling.

## Sentiments Automation 

This section illustrates sentiments towards automations.
For this section, we used 30.000 tweets in the time frame between the 19.05.2020 and the 27.05.2020.
First we compare different dictionary based methods.
To examine where people talk about automation, a map was creaeted with the location.
Unfortunately, the geospaital data in the tweets was limited.
Irrespective, the data illustrates that automation tweets accure for instance in the US more frequently in the country side and the southern states.
The graph can be found in the appendix.
Further looking at the sentiments towards automation, one can see sizable differences between dictionary methods.


```{r echo=FALSE,out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/nrc automation.png", "images/automation contribution.png" ))
``` 

Due to the lack of negative terms in the bing chart, we provide a second chart showing contribitions most frequently made per sentiment.

```{r echo=FALSE,out.width="49%", out.height="20%",fig.cap="Afinn and bing method entailing contribution per sentiment",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/afinn auto.png", "images/automation contribution bing.png"))
``` 
## Sentiments Migrants 

This section illustrates sentiments towards migration.
Examining where these tweets were posted, one can see that the borders were more likely to post tweets on the countries at the border.
Moreover, same holds for other countries tweeting about migration.
This suggests that these issues are talked about more frequently in areas where migration in a visible issue, while other geographic areas that are landlocked and far from borders don't put this issue at the center of attention.
The graph is attached in the appendix.

This section uses 45.000 tweets. 
Here, the period between the 


## Topic Modelling 

Compared to unigram and ngrams, topic models are mixed-membership models.
@grun_topicmodels_2011 further elucidate that unigrams and ngrams each word is drawn from the distribution of the topic. 
Topic models allow words to be ascosciated to multiple topics.
Lochard Ditricht Allocation is a bayesian mixture model, applicable to data for where topics are uncorrelated (@grun_topicmodels_2011).
<!-- Mixture models account for the exchangability of words and documents (Blei et al., 2003) -->
Further Blei ,Ng and Jordan point out that exchangibility does not imply indepedent and identically distributed but rather suggests a condional independence and indentical distribution. 
For this research the "topicmodels" is used, providing an extension to use correlated topic models (CTM).
As the name suggests, this model loosens this assumption of having uncorrelated topics, and allowd for topics to be correlated as argued by @grun_topicmodels_2011.
On a brief note, alpha should approximately be 50/k, k being the number of topics.
We define k to be 5.
Hence we define alpha as 10.

In the subsequent section one will discuss the topic groups for the four different terms.

### Employment

Looking at the employment chart, we can see a debate about how to find different solutions.
Due to the recent unemployment, there are more posts related to finding work.
Moreover, it appears that numerous post focus on law and policy.
Despite filtering for the words "employment" and "job", the word stem employ is depicted in the chart.
This illustrates that the filtering method and cleaning method could have been improved.

```{r echo=FALSE,out.width="80%", out.height="50%",fig.cap="LDA for employment",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/employment lda.png")
``` 

Especially topic 4 seems to focus on help by the state.
Generally it appears that little discussion for this keyword is focused on blaming groups.

### China 

Looking at the second chart focusing on China, one can see a different narrative.
Numerous topics center around Hong Kong, either depicted as "hong" or "kong" or both appearing as seen in topic 1,2,3 and 4.

```{r echo=FALSE,out.width="80%", out.height="50%",fig.cap="LDA for China",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/china lda.png" )
``` 


Further, numerous tweets related to president Donald Trump as seen in topic 2,4 and 5.
Also the term people appears numerous times.
But given that we dont know exactly in what context to which other terms, we would need insights into bigrams and trigrams to see in what context people was used. 
It could have been peoples republic of china, people of Hong Hong/Taiwan or people in context to America.
Employment and China dont seem to be the focus of the 5 topics in this chart.
To get more information on automation, lets look at the LDA for this term.


### Automation

Looking at automation, one can see that the center of attention is on how to use technology and automation to improve the future of work.
None of the topics appear to entail hateful sentiments.
Verbs such as use,can and learn appear frequently.

```{r echo=FALSE,out.width="80%", out.height="50%",fig.cap="LDA for automation",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/automation lda.png")
``` 

Generally speaking, these results hint towards a future oriented outlook rather than a narrative of fear and threat.
In light of the excess automation argument given by Acemolgu and XXX(XXX.), this may be something of interest to study in the long run.
Especially given the recent debate about the future of work and an amplification of automation due to the crisis and the exposed vulnerability of human workers.
This outcome corroborates the argument given by Acemoglu and Autor that there are overly positive sentiments towards technology relative to other threats.
Lastly, lets examine another frequently discussed issue.

### Migration 

While evidence is mixed, some topics suggest that the advantange of migrants is considered.
Illegal immgrant appreard both in topic 1 and topic 5.
Further, 

```{r echo=FALSE,out.width="80%", out.height="50%",fig.cap="LDA for migration",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/lda migration poli8sh.png")
``` 


For reference, the gibbs method and the CTM model are included in the appendix.

## Networks

To further visualize common topic debates in twitter data, we use a network graph.
Network graphs are used by creating ngrams, examinig words that frequently appear together.
This section examines both bi- and trigrams for numerous topics.

### Network: Automation 

```{r echo=FALSE,out.width="79%", out.height="50%",fig.cap="Trigram-network for automation ",fig.show='hold',fig.align='center'}
knitr::include_graphics(c( "images/fancy trigram auto.png"))
``` 


## Network: Migration 

```{r echo=FALSE,out.width="80%", out.height="50%",fig.cap="Trigram-network for migration",fig.show='hold',fig.align='center'}
knitr::include_graphics(c( "images/migration trigram.png"))
``` 

## Time series Plot

This section introduces an example of how to model changes in twitter frequency over time.
Prior research has used this type of plot to visualize historical events. 
Further research has also used to polarity changes over time.
This study could be extended to study for a longer period of time to study the impact of the economic lockdown in the long run.

```{r echo=FALSE,out.width="60%", out.height="40%",fig.cap="Time series for automation",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/hourly tweet automation.png" )
``` 



The second time series model shows how frequency of treets on migration changed.
One can see that there is spike between the 21st and 22nd of may.
At this period of time the US decided to extend their border policy.
Further, this illustration shows that looking at twitter data, one can examine different historic events.

```{r echo=FALSE,out.width="60%", out.height="40%",fig.cap="Time series for migration",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/corona extension border.png" )
``` 

Furthermore, future research could look at changes in polarity over time.
Looking at the 

It may make sense to look at the specific topics.
Some sentiments could be taken out of context.

# Conclusion
The assumption that technological change will continue in a linear manner is unlikely.
Technological change is volatile as are the repercussions of technology.
The assumptions made by in this paper are insightful for future research.
The study was unable to embed wage decline, unemployment or movements.
Irrespective, evidence suggest that the displacement effect and productive effect do interplay reshaping the landscape of the labor market.
Future research on how to accommodate these displaced workers is pivotal.
Especially given the increasing burden to pursue further education, policy makers need to provide guidance for both high, medium and low skilled workers.
Increasingly flexibility and technological change are part and parcel of our workforce and intend to stay.
Automation, artificial intelligence and the implementation of further software are inevitable predicaments of the labor market.
More research needs to focus on disentangling the relationship between technology and the rate of substitution of work.
While this relationship may appear linear on first sight, it clearly is bifurcated.
As mentioned by @webb_impact_2019, different technologies have a different impact on various groups.
Technological evolution and automation do not see class or color.
While AI may cause reduction in cost, demand potentially could also increase, creating higher labor demand.
This trend has not been studied sufficiently.

Further to complement these findings, this study attempted to disentangle fears and threats on social media.
Data on common political topics was collected to get insights into how political views might have shaped and what the current center of attention is.
Surprisingly, findings hint to a mixed perception of the recent unemployment wave.
There were numerous indicators that the stance also entailed hope rather than hate and blame. 
Further, automation did not manifest as a threat.
This research suggests that results can substantially deviate between topics.
Irrespective, the dictionary methods provide corroborating evidence that text analysis allows one to obtain a glimpse into social media trends. 
One should note that the results should be interpreted with caution.
Future mixed methods models, text classification regressions, generative models and word embeddings illustrate sizable potential within the literature.
These methods need to find further application in social sciences, to allow for more concise evidential support.

Jobs entailing a larger share of interpersonal and cognitive components are less prone to displacement due to technological innovation.
Workers within low skilled jobs are most likely to be exposed by robots.
Workers within middle income jobs are most likely to be exposed by software.
AI has an impact of jobs of the highly skilled.
Machine learning is designed to solve complex decision making, predicitive modeling and other advanced challenges.
Machine learning is an indication that most sectors of our economy will be exposed to technological change.
Policy makers need to accomodate this universal impact.

Further, the sentiment analysis allows us to get further insights into the feelings towards work, AI and immigrants.
One should note that these results are merely desciptive and dont ensure causality.


@webb_impact_2019 discovered 

# Discussion 

The labor market is changing severely. 
How we think about the future of work and what we think about threats to employment will shape the future political and inevitably, economic landscape.
This paper examined threats to the labor market among the general public using twitter data.
Examining common fears to employment, sentiments were strongest amongst tweets related to migration and China. 
Why does this matter?
As mentioned by Autor (2020), economic hardship has reshaped voting behaviour, hinting towards further partisanship. 
What we fear and what we perceive as a threat, shapes the narrative of politics and policy making.
In the rising age of political opportunism and political entrepreneurship, it is important that the voting population focuses on actual threats and fears rather than polarizing towards different narratives and following a resource allocation war between different groups.
As pointed out, numerous scholars have advocated for an endogenous evolution of technology.
Technological evolution is also a predicament inevitable to the future of work.
Hence, a critical stance towards the future of work is needed for a sustainable economic growth trajectory.
Therefore, our political and institutional corridor needs to accommodate a sustainable automation pace.
When looking at the social media data, excess automation does not manifest as a threat to employment.
While at this point it is not surprising that the economic lockdown and news on China manifest in the center of attention, future research needs to look into how technology is perceived in the long run.
A healthy relationship to automation and technology is pivotal to a growth sustaining society.
Rather than polarization attention to political topics such as foreign policy or migration, real threats needs to be disentangled and communicated. 
To sustain sustainable technological and ultimately economic growth, academia needs to communicate their ideas allowing them to unfold within political debate and manifest in voting behavior.
As pointed out by Autor, descriptive evidence hints towards the fact that trade with china has led to an increased polarization in states which have suffered economic hardship.
Future research needs to disentangle correlation and causation when it comes to technology and unemployment.
This study did not intend to provide causal analysis, but merely provided descriptive evidence towards what sentiments people have and what people are communicating via twitter.
Further, long-run changes towards certain topics need to be examined to further improve communication and understand common fears and threats.
Given the rise in relevance of social media more research needs to be done on the usage of bots to spread ideological predicaments.

# Appendix

## Tables:

Table 1b: **Occupations with highest and lowest exposure to robots**

| Least Exposed                     | Most Exposed                                 |
| --------------------------------- | -------------------------------------------- |
| Payroll clerks                    | Forklift drivers                             |
| Clergy                            | Operating engineers of cranes, derricks, etc.|
| Art/entertainment performers      | Elevator installers and repairers            |
| Correspondence and order clerks   | Janitors                                     |
| Eligibility clerks[^1]            | Locomotive operators: engineers and firemen  |


[^1]: Refined to eligibility clerks for government programs


 Table 2b: **Occupations with highest and lowest exposure to Software**

| Least Exposed                     | Most Exposed                                 |
| --------------------------------- | -------------------------------------------- |
| Barbers                           | Broadcast equipment operators                |
| Podiatrists                       | Water and sewage treatment plant operators   |
| Subject instructors, college      | Parking lot attendants                       |
| Art/entertainment performers      | Packers and packagers by hand                |
| Mail carriers for postal service  | Locomotive operators: engineers and firemen  |


Table 3b: **Occupations with highest and lowest exposure to Artifical Intelligence**

| Most Exposed                      | Least Exposed                                |
| --------------------------------- | -------------------------------------------- |
| Clinical laboratory technicians   | Animal caretaker, except farm                |
| Chemical engineers                | Food preparatiion workers                    |
| Optometrisits                     | Mail carriers for postal service             |
| Power plant operators             | Subject instructors, college                 |
| Dispatcher                        | Art/entertainment performers                 |


Table 4: **Regression table summary of exposure on wages (1;3) and employment (2;4)**

As one can see 


```{r echo=FALSE,out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/regression table summary .png" ))
``` 


## Topic Modelling 

```{r echo=FALSE,out.width="80%",out.height="50%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/gibbs migration.png" ))
``` 

## Network

```{r echo=FALSE,out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/fancy bigram auto.png", "images/network china.png", "images/bigram employment.png"))

```

### Mapping tweets

This section provides a map of the location of the twitter useres tweeting content related to the keyword migration.
As we have not specified tweets to be solely from america, but did use english tweets, we can see in what other countries this topic has gained prominance on twitter.
One can see that a considerable amount of tweets are coming from the UK and Africa. 
<!-- look up specific countries -->

```{r echo=FALSE,out.width="80%",out.height="50%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/automation map.png")
```

One can see that tweets about automation are centered differently compared to migration.

```{r echo=FALSE,out.width="80%",out.height="50%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics("images/maps/map migration tweets.png")
```
One can see that most of the tweets orginate from the US.
Irrespective some outliers do stick out. One can see that there are a sizable number of tweets from india.


### Network - China 

The network with bi- and tri-grams illustrate multivariate topics, relating to china in public debate.

```{r echo=FALSE,out.width="79%", out.height="50%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/network china.png" ))
``` 

Some of these topics are the separation movement in Hong Kong and Taiwan.
Further, "indian" and "armi" hint towards the border conflict in Ladakh.
Due to the 


### Networks: Employment

```{r echo=FALSE,out.width="79%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/employment good trigram.png" ))
``` 

The employment chart corroborates the findings suggested by the LDA.


# References
